{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyL9gZJmryPA",
        "outputId": "704c692a-ac4f-4a0a-b43a-5b7b36107f49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=ba1b6b53ac4090a46499a13d5ad2df45b65cca28917834f0ac7294869dacfe59\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -U transformers\n",
        "!pip install rouge-score nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UieOAwLJryPC"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFT5ForConditionalGeneration\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.bleu_score import sentence_bleu"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "The Warren Buffett Letters Q&A Enhanced Dataset (1998-2024) is a curated collection of question-answer pairs derived from Warren Buffett's annual shareholder letters to Berkshire Hathaway. The dataset spans 26 years of Buffett's investment wisdom and contains structured Q&A pairs extracted from real letter excerpts for conversational learning applications.\n",
        "\n",
        "**Column Descriptions:**\n",
        "- **question**: Financial queries covering investment philosophy, business analysis, market observations, and strategic decisions based on Buffett's letter content\n",
        "- **answer**: Expert-level responses reflecting Buffett's investment principles, analytical approach, and decision-making framework\n",
        "- **reasoning**: Detailed explanations of the underlying logic and investment principles behind each answer, providing context from Buffett's methodology\n",
        "\n",
        "The dataset was chosen for its authentic representation of Warren Buffett's investment philosophy and its structured format that enables effective fine-tuning of sequence-to-sequence models for financial advisory applications in the value investing domain."
      ],
      "metadata": {
        "id": "f3_Pfnch0OLL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8Ho6IEpryPC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eba4e67b-4475-494b-cb2e-1c156316558b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_parquet(\"hf://datasets/eagle0504/warren-buffett-letters-qna-r1-enhanced-1998-2024/data/train-00000-of-00001.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qlwhpq6NryPD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "a5ba4b66-2a8c-4d55-d0f6-da4e1ca3c50e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            question  \\\n",
              "0  **\"How does Warren Buffett's principle of 'pra...   \n",
              "1  Here are a few strong questions worth asking b...   \n",
              "2  **\"How does Warren Buffett's principle of 'pra...   \n",
              "3  **\"How does Warren Buffett's principle of 'pra...   \n",
              "4  **\"How does Warren Buffett's principle of 'pra...   \n",
              "\n",
              "                                              answer  \\\n",
              "0  A good answer would be:  \\n\\n*Warren Buffett e...   \n",
              "1  A good answer would be:  \\n\\nWarren Buffett ac...   \n",
              "2  Hereâ€™s a concise answer derived from the parag...   \n",
              "3  A good answer would highlight Warren Buffett's...   \n",
              "4  A good answer would highlight Warren Buffett's...   \n",
              "\n",
              "                                           reasoning  \n",
              "0  The reasoning is as follows:  \\n\\n1. **Context...  \n",
              "1  Warren Buffett emphasizes transparency, accoun...  \n",
              "2  The reasoning is as follows:  \\n\\n1. **Context...  \n",
              "3  The reasoning is as follows:  \\n\\n1. **Context...  \n",
              "4  The reasoning is as follows:  \\n\\n1. **Context...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-374ef408-f502-42a4-9ac6-c434e4ab91ea\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>reasoning</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>**\"How does Warren Buffett's principle of 'pra...</td>\n",
              "      <td>A good answer would be:  \\n\\n*Warren Buffett e...</td>\n",
              "      <td>The reasoning is as follows:  \\n\\n1. **Context...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Here are a few strong questions worth asking b...</td>\n",
              "      <td>A good answer would be:  \\n\\nWarren Buffett ac...</td>\n",
              "      <td>Warren Buffett emphasizes transparency, accoun...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>**\"How does Warren Buffett's principle of 'pra...</td>\n",
              "      <td>Hereâ€™s a concise answer derived from the parag...</td>\n",
              "      <td>The reasoning is as follows:  \\n\\n1. **Context...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>**\"How does Warren Buffett's principle of 'pra...</td>\n",
              "      <td>A good answer would highlight Warren Buffett's...</td>\n",
              "      <td>The reasoning is as follows:  \\n\\n1. **Context...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>**\"How does Warren Buffett's principle of 'pra...</td>\n",
              "      <td>A good answer would highlight Warren Buffett's...</td>\n",
              "      <td>The reasoning is as follows:  \\n\\n1. **Context...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-374ef408-f502-42a4-9ac6-c434e4ab91ea')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-374ef408-f502-42a4-9ac6-c434e4ab91ea button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-374ef408-f502-42a4-9ac6-c434e4ab91ea');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-9985d472-a341-439c-8475-d56a7013eb10\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9985d472-a341-439c-8475-d56a7013eb10')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-9985d472-a341-439c-8475-d56a7013eb10 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10657,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10079,\n        \"samples\": [\n          \"**\\\"How does GEICO's profit-sharing plan for employees compare to industry standards, and what impact does it have on employee retention and performance?\\\"**  \\n\\n**Alternatively:**  \\n**\\\"What factors contribute to GEICO's ability to maintain lower complaint ratios compared to other major auto insurers?\\\"**  \\n\\n**Or:**  \\n**\\\"Why does Berkshire Hathaway prioritize long-term value creation over short-term earnings when investing in GEICO's growth?\\\"**  \\n\\nWould you like a more specific or different type of question?\",\n          \"**\\\"How did Warren Buffett's hands-off approach to Pete Liegl's compensation and valuation contribute to the success of their business relationship?\\\"**  \\n\\nAlternatively:  \\n\\n**\\\"What does Warren Buffett's anecdote about Pete Liegl reveal about the importance of trust and simplicity in business deals?\\\"**  \\n\\nOr:  \\n\\n**\\\"Why did Warren Buffett appreciate Pete Liegl's straightforward negotiation style, and how did it benefit Berkshire Hathaway?\\\"**\",\n          \"**\\\"How has Berkshire Hathaway Energy's focus on wind power contributed to its competitive advantage in electricity rates compared to other utilities?\\\"**  \\n\\n**OR**  \\n\\n**\\\"What factors have enabled GUARD Insurance Group to achieve such significant growth under Berkshire Hathaway's ownership?\\\"**  \\n\\n**OR**  \\n\\n**\\\"Why does Berkshire Hathaway Energy retain earnings rather than paying large dividends like other utility companies?\\\"**  \\n\\n*(Choose the one that aligns best with your interest!)*\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10137,\n        \"samples\": [\n          \"Here\\u2019s a concise answer based on the paragraph and question:  \\n\\n*Warren Buffett and Charlie Munger structured their investment partnerships with a strong emphasis on trust, treating their partners' money as their own, and maintaining an extreme aversion to permanent capital loss. Their approach was rooted in a long-term, partnership-like mindset, even as their operations evolved into Berkshire Hathaway. Today, Berkshire's shareholders\\u2014whether index funds, active managers, or individual investors\\u2014reflect diverse strategies, but Buffett and Munger continue prioritizing alignment with long-term partners who share their principles.*\",\n          \"**Question:**  \\n*How did Berkshire Hathaway's compounded annual gain compare to the S&P 500 from 1965 to 2016?*  \\n\\n**Answer:**  \\nBerkshire Hathaway's compounded annual gain (19.0% for book value, 20.8% for market value) significantly outperformed the S&P 500's 9.7% gain over the same period.\",\n          \"**Answer:** The Clayton home, priced at \\\\$139,900, features products from Berkshire subsidiaries like Acme brick, Shaw carpet, and NFM furniture, offering excellent value. Last year, one of the displayed homes was sold before the meeting even began.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reasoning\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10647,\n        \"samples\": [\n          \"The reasoning involves analyzing Warren Buffett's emphasis on Berkshire Hathaway's long-term reinvestment strategy and its impact on tax payments. The paragraph highlights:  \\n\\n1. **Minimal Dividends, Massive Reinvestment** \\u2013 Berkshire paid only one small dividend in 1967, allowing capital to compound over decades.  \\n2. **Tax Contributions** \\u2013 Reinvestment led to enormous taxable income, with $101+ billion paid in taxes, including $26.8 billion in 2024 alone.  \\n3. **Investment Strategy** \\u2013 Buffett explains Berkshire\\u2019s dual approach: wholly owned subsidiaries and minority stakes in high-quality public companies, both chosen based on value.  \\n\\nThe answer connects these points: **Reinvesting profits (instead of paying dividends) allowed Berkshire to compound capital, generating far greater taxable income over time, while flexibility in equity investments (control vs. minority stakes) maximized returns.** This aligns with Buffett\\u2019s philosophy of long-term value creation.\",\n          \"Warren Buffett highlights the increasing risks and financial challenges faced by utilities due to climate change (e.g., wildfires, regulatory uncertainty) and acknowledges Berkshire's costly oversight in anticipating these issues. However, he contrasts this with the strong performance of Berkshire's insurance business, emphasizing disciplined underwriting, risk awareness, and long-term resilience. The key takeaway is that while some industries (like utilities) face existential threats, Berkshire's diversified structure\\u2014particularly its insurance operations\\u2014positions it to endure financial shocks by avoiding optimism in risk assessment and maintaining rigorous underwriting standards.\",\n          \"The reasoning is as follows:  \\n\\n1. **Context from the Paragraph**: Warren Buffett explains that Berkshire Hathaway stopped its parent-level charitable giving program because it inadvertently harmed independent associates of The Pampered Chef (a Berkshire subsidiary) who faced boycotts and income losses due to Berkshire's donations.  \\n\\n2. **Key Justification**: Buffett and Charlie Brown prioritized fairness over minor tax benefits, stating that harming hard-working associates for shareholder tax efficiency was not charitable.  \\n\\n3. **Answer Derivation**: The best answer aligns with Buffett\\u2019s ethical stance\\u2014Berkshire stopped the program to avoid harming its business partners, even if it meant losing tax advantages. This reflects their principle of protecting stakeholders over financial optimization.  \\n\\nThus, the answer logically follows Buffett\\u2019s reasoning in the text.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model\n",
        "\n",
        "FLAN-T5 is an enhanced version of Google's T5 (Text-to-Text Transfer Transformer) architecture. It extends the original T5 by fine-tuning on over 1,000 additional tasks across multiple languages, creating a more robust foundation model for natural language processing applications.\n",
        "\n",
        "**Key advantages of FLAN-T5:**\n",
        "- **Multilingual capabilities**: Trained on diverse language datasets for broader applicability  \n",
        "- **Sequence-to-sequence excellence**: Proven effectiveness for conditional text generation tasks\n",
        "- **Large-scale pre-training**: Extensive training corpus ensures strong foundational knowledge\n",
        "\n",
        "FLAN-T5 was selected as the base model due to its demonstrated performance on sequence-to-sequence tasks and its ability to follow instructions effectively, making it well-suited for question-answering applications.\n",
        "\n",
        "**Base Model**: [google/flan-t5-base](https://huggingface.co/google/flan-t5-base)"
      ],
      "metadata": {
        "id": "zJp4w0ZU0S8J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXfPXQ05ryPE"
      },
      "outputs": [],
      "source": [
        "def load_tokenizer(model_name=\"google/flan-t5-base\"):\n",
        "    \"\"\"\n",
        "    Loads tokenizer for T5 model from HuggingFace Hub.\n",
        "\n",
        "    Args:\n",
        "        model_name: HuggingFace model identifier (default: \"google/flan-t5-base\")\n",
        "\n",
        "    Returns:\n",
        "        AutoTokenizer: Pre-trained tokenizer matching the specified model\n",
        "    \"\"\"\n",
        "    # Load compatible tokenizer for text preprocessing\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    return tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GSLySfEryPE"
      },
      "outputs": [],
      "source": [
        "def load_model(model_name=\"google/flan-t5-base\"):\n",
        "    \"\"\"\n",
        "    Loads pre-trained T5 model from HuggingFace Hub.\n",
        "\n",
        "    Args:\n",
        "        model_name: HuggingFace model identifier (default: \"google/flan-t5-base\")\n",
        "\n",
        "    Returns:\n",
        "        TFT5ForConditionalGeneration: Loaded TensorFlow T5 model for conditional generation\n",
        "    \"\"\"\n",
        "    # Load pre-trained T5 model for sequence-to-sequence tasks\n",
        "    model = TFT5ForConditionalGeneration.from_pretrained(model_name)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing\n",
        "\n",
        "The dataset undergoes comprehensive preprocessing to ensure high-quality training data for the sequence-to-sequence model. The preprocessing pipeline consists of dataset combination, text cleaning, and quality filtering.\n",
        "\n",
        "**Dataset Enhancement:**\n",
        "The original financial Q&A data is combined with 75+ out-of-domain questions paired with standardized rejection responses, teaching the model to recognize and politely decline non-financial queries.\n",
        "\n",
        "**Text Cleaning:**\n",
        "Removes markdown formatting, escaped characters, and annotations while preserving content. Normalizes whitespace and extracts relevant questions from formatted text to create clean, uniform input.\n",
        "\n",
        "**Quality Filtering:**\n",
        "Eliminates entries with missing data and enforces minimum length requirements for financial content while preserving all rejection examples for robust domain boundary learning.\n",
        "\n",
        "This preprocessing approach creates clean input that enables effective model training while teaching proper domain boundaries and preserving Buffett's investment insights."
      ],
      "metadata": {
        "id": "8EU9YLaQ2kE_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Asa0a_AQryPD"
      },
      "outputs": [],
      "source": [
        "def preprocess_for_chatbot(text):\n",
        "    \"\"\"\n",
        "    Preprocesses text for chatbot input by cleaning formatting and extracting questions.\n",
        "\n",
        "    Args:\n",
        "        text: Raw text input (may contain markdown, escape characters, annotations)\n",
        "\n",
        "    Returns:\n",
        "        str: Cleaned text ready for chatbot processing\n",
        "    \"\"\"\n",
        "    # Handle missing or null values\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "\n",
        "    text = str(text)\n",
        "\n",
        "    # Replace escaped characters for readability\n",
        "    text = text.replace(\"\\\\n\", \" \").replace(\"\\\\t\", \" \").replace(\"\\\\'\", \"'\").replace('\\\\\"', '\"')\n",
        "\n",
        "    # Extract first question from bold markdown format\n",
        "    questions = re.findall(r'\\*\\*\"([^\"]*?)\"\\*\\*', text)\n",
        "    if questions:\n",
        "        return questions[0].strip()\n",
        "\n",
        "    # Remove markdown formatting\n",
        "    text = re.sub(r'\\*\\*(.*?)\\*\\*', r'\\1', text)  # **text** â†’ text\n",
        "    text = re.sub(r'\\*(.*?)\\*', r'\\1', text)      # *text* â†’ text\n",
        "\n",
        "    # Remove annotations in parentheses\n",
        "    text = re.sub(r'\\*\\([^)]*\\)\\*', '', text)\n",
        "\n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Remove surrounding quotes if they wrap the entire text\n",
        "    if text.startswith('\"') and text.endswith('\"') and text.count('\"') == 2:\n",
        "        text = text[1:-1].strip()\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGmGb01FryPD"
      },
      "outputs": [],
      "source": [
        "def preprocess_chatbot_dataset(df):\n",
        "    \"\"\"\n",
        "    Preprocesses the entire chatbot dataset by cleaning text and filtering entries.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame containing question-answer pairs with optional reasoning\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: Cleaned dataset ready for model training\n",
        "    \"\"\"\n",
        "    # Apply text cleaning to all relevant columns\n",
        "    for col in ['question', 'reasoning', 'answer']:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].apply(preprocess_for_chatbot)\n",
        "\n",
        "    # Remove entries with missing critical data\n",
        "    df = df.dropna(subset=['question', 'answer'])\n",
        "\n",
        "    # Filter out entries that are too short to be meaningful\n",
        "    df = df[df['question'].str.len() > 5]\n",
        "    df = df[df['answer'].str.len() > 10]\n",
        "\n",
        "    print(f\"Dataset ready: {len(df)} samples\")\n",
        "\n",
        "    # Display sample data for verification\n",
        "    if len(df) > 0:\n",
        "        print(f\"\\nSample preserved question:\")\n",
        "        print(f\"'{df['question'].iloc[0]}'\")\n",
        "        print(f\"\\nSample preserved answer:\")\n",
        "        print(f\"'{df['answer'].iloc[0]}'\")\n",
        "        print(f\"\\nSample preserved reasoning:\")\n",
        "        print(f\"'{df['reasoning'].iloc[0]}'\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1FURQvvryPF"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def split_train_val(df, val_size=0.1):\n",
        "    \"\"\"\n",
        "    Splits dataset into training and validation sets.\n",
        "\n",
        "    Args:\n",
        "        df: Input DataFrame to split\n",
        "        val_size: Fraction for validation set (default: 0.2)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (train_df, val_df) - Training and validation DataFrames\n",
        "    \"\"\"\n",
        "    # Split with fixed random seed\n",
        "    train_df, val_df = train_test_split(df, test_size=val_size, random_state=42)\n",
        "\n",
        "    return train_df, val_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EI8C8kJ4ryPF"
      },
      "outputs": [],
      "source": [
        "def tokenize_data(df, tokenizer, batch_size=16):\n",
        "    \"\"\"\n",
        "    Tokenizes question-answer pairs and returns a batched tf.data.Dataset.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame with 'question' and 'answer' columns\n",
        "        tokenizer: Pre-trained tokenizer (e.g., from transformers library)\n",
        "        batch_size: Number of samples per batch (default: 4)\n",
        "\n",
        "    Returns:\n",
        "        tf.data.Dataset: Batched dataset with input_ids, attention_mask, and labels\n",
        "    \"\"\"\n",
        "    inputs = []\n",
        "    labels = []\n",
        "    attention_masks = []\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        # Add context prompt during training (same as inference)\n",
        "        input_text = f\"Answer this financial question based on Warren Buffett's principles: {row['question']}\"\n",
        "\n",
        "        question_tokens = tokenizer(\n",
        "            input_text,  # Use formatted input instead of raw question\n",
        "            max_length=256,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='tf'\n",
        "        )\n",
        "        inputs.append(question_tokens['input_ids'][0])\n",
        "        attention_masks.append(question_tokens['attention_mask'][0])\n",
        "\n",
        "        # Tokenize answer\n",
        "        answer_tokens = tokenizer(\n",
        "            row['answer'],\n",
        "            max_length=256,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='tf'\n",
        "        )\n",
        "        labels.append(answer_tokens['input_ids'][0])\n",
        "\n",
        "    # Convert to tensors\n",
        "    inputs = tf.stack(inputs)\n",
        "    attention_masks = tf.stack(attention_masks)\n",
        "    labels = tf.stack(labels)\n",
        "\n",
        "    # Create and return batched dataset\n",
        "    return tf.data.Dataset.from_tensor_slices({\n",
        "        'input_ids': inputs,\n",
        "        'attention_mask': attention_masks,\n",
        "        'labels': labels\n",
        "    }).batch(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the out-of-domain data\n",
        "out_of_domain_df = pd.read_csv('out_of_domain_training_data.csv')\n",
        "\n",
        "# Combine both datasets\n",
        "enhanced_df = pd.concat([df, out_of_domain_df], ignore_index=True)\n",
        "\n",
        "# Shuffle the combined dataset\n",
        "enhanced_df = enhanced_df.sample(frac=1, random_state=42).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "EGPI8jxt0F0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = preprocess_chatbot_dataset(enhanced_df)"
      ],
      "metadata": {
        "id": "NbG0gvmJ9k37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9640ab4-6ec3-47fe-e3fb-1d6700bf321a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset ready: 10741 samples\n",
            "\n",
            "Sample preserved question:\n",
            "'What's a good recipe for pizza?'\n",
            "\n",
            "Sample preserved answer:\n",
            "'I'm designed to answer questions about Warren Buffett's investment principles and philosophy. Could you please ask a question related to investing, business analysis, or financial strategy?'\n",
            "\n",
            "Sample preserved reasoning:\n",
            "'Out-of-domain query requiring polite redirection to financial topics'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dkyfiak8ryPF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84c10215-4e7a-4844-e2d0-c0883edec76a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train samples: 9666\n",
            "Val samples: 1075\n"
          ]
        }
      ],
      "source": [
        "train_df, val_df = split_train_val(df)\n",
        "\n",
        "print(f\"Train samples: {len(train_df)}\")\n",
        "print(f\"Val samples: {len(val_df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "CNHZWGM-ryPF"
      },
      "outputs": [],
      "source": [
        "def train_model(train_dict, val_dict, learning_rate=5.8e-5, callbacks=[], epochs=30):\n",
        "    \"\"\"\n",
        "    Fine-tunes FLAN-T5 model on question-answer pairs.\n",
        "\n",
        "    Args:\n",
        "        train_dict: Training dataset (tf.data.Dataset with input_ids, attention_mask, labels)\n",
        "        val_dict: Validation dataset (tf.data.Dataset with same structure)\n",
        "        learning_rate: Adam optimizer learning rate (default: 5e-5)\n",
        "        callbacks: List of Keras callbacks for training monitoring (default: [])\n",
        "        epochs: Number of training epochs (default: 35)\n",
        "\n",
        "    Returns:\n",
        "        TFT5ForConditionalGeneration: Fine-tuned model ready for inference\n",
        "    \"\"\"\n",
        "    # Load pre-trained FLAN-T5 base model\n",
        "    model = load_model()\n",
        "\n",
        "    # Configure Adam optimizer with specified learning rate\n",
        "    optimizer = tf.keras.optimizers.AdamW(\n",
        "        learning_rate=learning_rate,\n",
        "        weight_decay=0.01\n",
        "        )\n",
        "    model.compile(optimizer=optimizer)\n",
        "\n",
        "    # Fine-tune model with callbacks for training monitoring\n",
        "    model.fit(\n",
        "        train_dict,\n",
        "        validation_data=val_dict,\n",
        "        epochs=epochs,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_simple_callbacks():\n",
        "    \"\"\"\n",
        "    Creates standard Keras callbacks for training optimization and monitoring.\n",
        "\n",
        "    Returns:\n",
        "        list: Configured callbacks for early stopping, learning rate reduction, and model checkpointing\n",
        "    \"\"\"\n",
        "    # Training hyperparameters - easily adjustable\n",
        "    PATIENCE = 3             # Epochs to wait before early stopping\n",
        "    LR_REDUCTION_FACTOR = 0.3 # Factor to reduce learning rate by\n",
        "    LR_PATIENCE = 1           # Epochs to wait before reducing learning rate\n",
        "    MIN_LR = 1e-8            # Minimum learning rate threshold\n",
        "    MIN_DELTA = 0.001         # Minimum change to qualify as improvement\n",
        "    RESTORE_BEST = True      # Restore best weights on early stop\n",
        "\n",
        "    callbacks = [\n",
        "        # Prevent overfitting by stopping training when validation loss plateaus\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=PATIENCE,\n",
        "            restore_best_weights=RESTORE_BEST,\n",
        "            min_delta=MIN_DELTA,\n",
        "            verbose=1\n",
        "        ),\n",
        "\n",
        "        # Adaptive learning rate reduction when training stagnates\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=LR_REDUCTION_FACTOR,\n",
        "            patience=LR_PATIENCE,\n",
        "            min_lr=MIN_LR,\n",
        "            min_delta=MIN_DELTA,\n",
        "            cooldown=1,              # Wait 1 epoch after reduction\n",
        "            verbose=1\n",
        "        ),\n",
        "\n",
        "        # tf.keras.callbacks.LearningRateScheduler(\n",
        "        #     lambda epoch: 3e-5 * (0.9 ** epoch)\n",
        "        # ),\n",
        "\n",
        "        # Save best performing model during training\n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            filepath='./best_model.keras',\n",
        "            monitor='val_loss',\n",
        "            save_best_only=True,\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    return callbacks"
      ],
      "metadata": {
        "id": "T2rB_Vuf-wyI"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "rQfbsPm8ryPF"
      },
      "outputs": [],
      "source": [
        "tokenizer = load_tokenizer()\n",
        "\n",
        "train_dict = tokenize_data(train_df, tokenizer)\n",
        "val_dict = tokenize_data(val_df, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vt6MY891ryPF",
        "outputId": "ac450c2a-bbbe-4b98-f868-5891bb1701c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
            "\n",
            "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "605/605 [==============================] - ETA: 0s - loss: 1.8589\n",
            "Epoch 1: val_loss improved from inf to 0.80000, saving model to ./best_model.keras\n",
            "605/605 [==============================] - 257s 309ms/step - loss: 1.8589 - val_loss: 0.8000 - lr: 5.8000e-05\n",
            "Epoch 2/30\n",
            "605/605 [==============================] - ETA: 0s - loss: 0.8279\n",
            "Epoch 2: val_loss improved from 0.80000 to 0.72377, saving model to ./best_model.keras\n",
            "605/605 [==============================] - 167s 276ms/step - loss: 0.8279 - val_loss: 0.7238 - lr: 5.8000e-05\n",
            "Epoch 3/30\n",
            "605/605 [==============================] - ETA: 0s - loss: 0.7576\n",
            "Epoch 3: val_loss improved from 0.72377 to 0.66987, saving model to ./best_model.keras\n",
            "605/605 [==============================] - 167s 276ms/step - loss: 0.7576 - val_loss: 0.6699 - lr: 5.8000e-05\n",
            "Epoch 4/30\n",
            "605/605 [==============================] - ETA: 0s - loss: 0.7047\n",
            "Epoch 4: val_loss improved from 0.66987 to 0.62526, saving model to ./best_model.keras\n",
            "605/605 [==============================] - 167s 276ms/step - loss: 0.7047 - val_loss: 0.6253 - lr: 5.8000e-05\n",
            "Epoch 5/30\n",
            "605/605 [==============================] - ETA: 0s - loss: 0.6599\n",
            "Epoch 5: val_loss improved from 0.62526 to 0.58770, saving model to ./best_model.keras\n",
            "605/605 [==============================] - 167s 276ms/step - loss: 0.6599 - val_loss: 0.5877 - lr: 5.8000e-05\n",
            "Epoch 6/30\n",
            "605/605 [==============================] - ETA: 0s - loss: 0.6210\n",
            "Epoch 6: val_loss improved from 0.58770 to 0.55530, saving model to ./best_model.keras\n",
            "605/605 [==============================] - 166s 275ms/step - loss: 0.6210 - val_loss: 0.5553 - lr: 5.8000e-05\n",
            "Epoch 7/30\n",
            "605/605 [==============================] - ETA: 0s - loss: 0.5861\n",
            "Epoch 7: val_loss improved from 0.55530 to 0.52648, saving model to ./best_model.keras\n",
            "605/605 [==============================] - 167s 276ms/step - loss: 0.5861 - val_loss: 0.5265 - lr: 5.8000e-05\n",
            "Epoch 8/30\n",
            "605/605 [==============================] - ETA: 0s - loss: 0.5539\n",
            "Epoch 8: val_loss improved from 0.52648 to 0.50130, saving model to ./best_model.keras\n",
            "605/605 [==============================] - 166s 275ms/step - loss: 0.5539 - val_loss: 0.5013 - lr: 5.8000e-05\n",
            "Epoch 9/30\n",
            "605/605 [==============================] - ETA: 0s - loss: 0.5254\n",
            "Epoch 9: val_loss improved from 0.50130 to 0.48004, saving model to ./best_model.keras\n",
            "605/605 [==============================] - 166s 275ms/step - loss: 0.5254 - val_loss: 0.4800 - lr: 5.8000e-05\n",
            "Epoch 10/30\n",
            "605/605 [==============================] - ETA: 0s - loss: 0.5003\n",
            "Epoch 10: val_loss improved from 0.48004 to 0.46100, saving model to ./best_model.keras\n",
            "605/605 [==============================] - 166s 275ms/step - loss: 0.5003 - val_loss: 0.4610 - lr: 5.8000e-05\n",
            "Epoch 11/30\n",
            "605/605 [==============================] - ETA: 0s - loss: 0.4761\n",
            "Epoch 11: val_loss improved from 0.46100 to 0.44465, saving model to ./best_model.keras\n",
            "605/605 [==============================] - 167s 275ms/step - loss: 0.4761 - val_loss: 0.4446 - lr: 5.8000e-05\n",
            "Epoch 12/30\n",
            "605/605 [==============================] - ETA: 0s - loss: 0.4550\n",
            "Epoch 12: val_loss improved from 0.44465 to 0.43042, saving model to ./best_model.keras\n",
            "605/605 [==============================] - 166s 275ms/step - loss: 0.4550 - val_loss: 0.4304 - lr: 5.8000e-05\n",
            "Epoch 13/30\n",
            "605/605 [==============================] - ETA: 0s - loss: 0.4355\n",
            "Epoch 13: val_loss improved from 0.43042 to 0.41787, saving model to ./best_model.keras\n",
            "605/605 [==============================] - 167s 276ms/step - loss: 0.4355 - val_loss: 0.4179 - lr: 5.8000e-05\n",
            "Epoch 14/30\n",
            "605/605 [==============================] - ETA: 0s - loss: 0.4174\n",
            "Epoch 14: val_loss improved from 0.41787 to 0.40672, saving model to ./best_model.keras\n",
            "605/605 [==============================] - 167s 275ms/step - loss: 0.4174 - val_loss: 0.4067 - lr: 5.8000e-05\n",
            "Epoch 15/30\n",
            "605/605 [==============================] - ETA: 0s - loss: 0.4004\n",
            "Epoch 15: val_loss improved from 0.40672 to 0.39691, saving model to ./best_model.keras\n",
            "605/605 [==============================] - 166s 275ms/step - loss: 0.4004 - val_loss: 0.3969 - lr: 5.8000e-05\n",
            "Epoch 16/30\n",
            "605/605 [==============================] - ETA: 0s - loss: 0.3849\n",
            "Epoch 16: val_loss improved from 0.39691 to 0.38777, saving model to ./best_model.keras\n",
            "605/605 [==============================] - 168s 277ms/step - loss: 0.3849 - val_loss: 0.3878 - lr: 5.8000e-05\n",
            "Epoch 17/30\n",
            "605/605 [==============================] - ETA: 0s - loss: 0.3707\n",
            "Epoch 17: val_loss improved from 0.38777 to 0.38041, saving model to ./best_model.keras\n",
            "605/605 [==============================] - 166s 275ms/step - loss: 0.3707 - val_loss: 0.3804 - lr: 5.8000e-05\n",
            "Epoch 18/30\n",
            "605/605 [==============================] - ETA: 0s - loss: 0.3574\n",
            "Epoch 18: val_loss improved from 0.38041 to 0.37450, saving model to ./best_model.keras\n",
            "605/605 [==============================] - 167s 276ms/step - loss: 0.3574 - val_loss: 0.3745 - lr: 5.8000e-05\n",
            "Epoch 19/30\n",
            "605/605 [==============================] - ETA: 0s - loss: 0.3447\n",
            "Epoch 19: val_loss improved from 0.37450 to 0.36751, saving model to ./best_model.keras\n",
            "605/605 [==============================] - 167s 276ms/step - loss: 0.3447 - val_loss: 0.3675 - lr: 5.8000e-05\n",
            "Epoch 20/30\n",
            "605/605 [==============================] - ETA: 0s - loss: 0.3329\n",
            "Epoch 20: val_loss improved from 0.36751 to 0.36368, saving model to ./best_model.keras\n",
            "605/605 [==============================] - 169s 279ms/step - loss: 0.3329 - val_loss: 0.3637 - lr: 5.8000e-05\n",
            "Epoch 21/30\n",
            "605/605 [==============================] - ETA: 0s - loss: 0.3213\n",
            "Epoch 21: val_loss improved from 0.36368 to 0.35867, saving model to ./best_model.keras\n",
            "605/605 [==============================] - 167s 276ms/step - loss: 0.3213 - val_loss: 0.3587 - lr: 5.8000e-05\n",
            "Epoch 22/30\n",
            "605/605 [==============================] - ETA: 0s - loss: 0.3111\n",
            "Epoch 22: val_loss improved from 0.35867 to 0.35417, saving model to ./best_model.keras\n",
            "605/605 [==============================] - 166s 275ms/step - loss: 0.3111 - val_loss: 0.3542 - lr: 5.8000e-05\n",
            "Epoch 23/30\n",
            "605/605 [==============================] - ETA: 0s - loss: 0.3013\n",
            "Epoch 23: val_loss improved from 0.35417 to 0.35072, saving model to ./best_model.keras\n",
            "605/605 [==============================] - 167s 276ms/step - loss: 0.3013 - val_loss: 0.3507 - lr: 5.8000e-05\n",
            "Epoch 24/30\n",
            "605/605 [==============================] - ETA: 0s - loss: 0.2915\n",
            "Epoch 24: val_loss improved from 0.35072 to 0.34825, saving model to ./best_model.keras\n",
            "605/605 [==============================] - 166s 275ms/step - loss: 0.2915 - val_loss: 0.3483 - lr: 5.8000e-05\n",
            "Epoch 25/30\n",
            "605/605 [==============================] - ETA: 0s - loss: 0.2826\n",
            "Epoch 25: val_loss improved from 0.34825 to 0.34453, saving model to ./best_model.keras\n",
            "605/605 [==============================] - 167s 276ms/step - loss: 0.2826 - val_loss: 0.3445 - lr: 5.8000e-05\n",
            "Epoch 26/30\n",
            "605/605 [==============================] - ETA: 0s - loss: 0.2743\n",
            "Epoch 26: val_loss improved from 0.34453 to 0.34168, saving model to ./best_model.keras\n",
            "605/605 [==============================] - 166s 275ms/step - loss: 0.2743 - val_loss: 0.3417 - lr: 5.8000e-05\n",
            "Epoch 27/30\n",
            "605/605 [==============================] - ETA: 0s - loss: 0.2655\n",
            "Epoch 27: val_loss improved from 0.34168 to 0.33961, saving model to ./best_model.keras\n",
            "605/605 [==============================] - 167s 276ms/step - loss: 0.2655 - val_loss: 0.3396 - lr: 5.8000e-05\n",
            "Epoch 28/30\n",
            "605/605 [==============================] - ETA: 0s - loss: 0.2585\n",
            "Epoch 28: ReduceLROnPlateau reducing learning rate to 1.7400001524947584e-05.\n",
            "\n",
            "Epoch 28: val_loss improved from 0.33961 to 0.33900, saving model to ./best_model.keras\n",
            "605/605 [==============================] - 166s 275ms/step - loss: 0.2585 - val_loss: 0.3390 - lr: 5.8000e-05\n",
            "Epoch 29/30\n",
            "605/605 [==============================] - ETA: 0s - loss: 0.2473\n",
            "Epoch 29: ReduceLROnPlateau reducing learning rate to 5.220000730332686e-06.\n",
            "\n",
            "Epoch 29: val_loss did not improve from 0.33900\n",
            "605/605 [==============================] - 154s 255ms/step - loss: 0.2473 - val_loss: 0.3401 - lr: 1.7400e-05\n",
            "Epoch 30/30\n",
            "605/605 [==============================] - ETA: 0s - loss: 0.2428\n",
            "Epoch 30: ReduceLROnPlateau reducing learning rate to 1.5660002645745408e-06.\n",
            "\n",
            "Epoch 30: val_loss did not improve from 0.33900\n",
            "605/605 [==============================] - 154s 255ms/step - loss: 0.2428 - val_loss: 0.3398 - lr: 5.2200e-06\n",
            "Epoch 30: early stopping\n",
            "Restoring model weights from the end of the best epoch: 27.\n"
          ]
        }
      ],
      "source": [
        "callbacks = get_simple_callbacks()\n",
        "\n",
        "model = train_model(train_dict, val_dict, callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Vr5TT4rrryPF"
      },
      "outputs": [],
      "source": [
        "question = \"How often do you review your portfolio\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "eZPIakPR34uX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "181ed849-e02a-4ebe-fcdd-655476a85332"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question: How often do you review your portfolio\n",
            "Predicted Answer: How does Berkshire ensure that the questions selected by analysts and journalists truly reflect the most pressing concerns of shareholders, rather than just the interests of the financial media?\n"
          ]
        }
      ],
      "source": [
        "def predict_answer(question, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Generates answers to financial questions using fine-tuned FLAN-T5 model.\n",
        "\n",
        "    Args:\n",
        "        question: Input question text\n",
        "        model: Fine-tuned TFT5ForConditionalGeneration model\n",
        "        tokenizer: Corresponding T5 tokenizer\n",
        "\n",
        "    Returns:\n",
        "        str: Generated answer following Warren Buffett's investment principles\n",
        "    \"\"\"\n",
        "    # Format input with same context prompt used during training\n",
        "    input_text = f\"Answer this financial question based on Warren Buffett's principles: {question}\"\n",
        "\n",
        "    # Tokenize input with padding and truncation for consistent format\n",
        "    input_tokens = tokenizer(\n",
        "        input_text,\n",
        "        return_tensors=\"tf\",\n",
        "        max_length=256,\n",
        "        padding='max_length',\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    # Generate response using beam search for higher quality output\n",
        "    generated_tokens = model.generate(\n",
        "        input_tokens[\"input_ids\"],\n",
        "        attention_mask=input_tokens[\"attention_mask\"],\n",
        "        max_length=256,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    # Decode generated tokens to readable text\n",
        "    predicted_answer = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
        "\n",
        "    return predicted_answer\n",
        "\n",
        "# Example usage\n",
        "predicted_answer = predict_answer(question, model, tokenizer)\n",
        "print(f\"\\nQuestion: {question}\")\n",
        "print(f\"Predicted Answer: {predicted_answer}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "1s4LfG6z34uY"
      },
      "outputs": [],
      "source": [
        "def calculate_bleu(reference, candidate):\n",
        "    \"\"\"\n",
        "    Calculates BLEU score to measure similarity between reference and generated text.\n",
        "\n",
        "    Args:\n",
        "        reference: Ground truth answer text\n",
        "        candidate: Model-generated answer text\n",
        "\n",
        "    Returns:\n",
        "        float: BLEU score between 0 and 1 (higher = better similarity)\n",
        "    \"\"\"\n",
        "    # Tokenize texts by splitting on whitespace\n",
        "    reference_tokens = reference.split()\n",
        "    candidate_tokens = candidate.split()\n",
        "\n",
        "    # Calculate sentence-level BLEU score\n",
        "    return sentence_bleu([reference_tokens], candidate_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "9QFcp45c34uY"
      },
      "outputs": [],
      "source": [
        "def calculate_rouge(reference, candidate):\n",
        "    \"\"\"\n",
        "    Calculates ROUGE scores to evaluate text generation quality.\n",
        "\n",
        "    Args:\n",
        "        reference: Ground truth answer text\n",
        "        candidate: Model-generated answer text\n",
        "\n",
        "    Returns:\n",
        "        dict: ROUGE scores with keys 'rouge1', 'rouge2', 'rougeL' (0-1 scale, higher = better)\n",
        "    \"\"\"\n",
        "    # Initialize ROUGE scorer with stemming for better matching\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "    # Calculate all ROUGE variants\n",
        "    scores = scorer.score(reference, candidate)\n",
        "\n",
        "    # Return F1 scores for each ROUGE metric\n",
        "    return {\n",
        "        'rouge1': scores['rouge1'].fmeasure,   # Unigram overlap\n",
        "        'rouge2': scores['rouge2'].fmeasure,   # Bigram overlap\n",
        "        'rougeL': scores['rougeL'].fmeasure    # Longest common subsequence\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "pmW0JoeYEVRA"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def calculate_perplexity(model, tokenizer, text, max_length=256):\n",
        "    \"\"\"\n",
        "    Calculates perplexity to measure how well the model predicts the given text.\n",
        "\n",
        "    Args:\n",
        "        model: Fine-tuned language model\n",
        "        tokenizer: Corresponding tokenizer\n",
        "        text: Input text to evaluate\n",
        "        max_length: Maximum sequence length for tokenization (default: 256)\n",
        "\n",
        "    Returns:\n",
        "        float: Perplexity score (lower = better, inf if calculation fails)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Tokenize input text with truncation for consistent length\n",
        "        inputs = tokenizer(text, return_tensors='tf', max_length=max_length, truncation=True)\n",
        "        input_ids = inputs['input_ids']\n",
        "\n",
        "        # Calculate loss using input as both input and target (language modeling)\n",
        "        outputs = model(input_ids, labels=input_ids)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Convert cross-entropy loss to perplexity\n",
        "        perplexity = math.exp(loss.numpy())\n",
        "        return perplexity\n",
        "\n",
        "    except:\n",
        "        # Return infinity if calculation fails (e.g., empty text, model errors)\n",
        "        return float('inf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "4MGAR0Ao34uY"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, tokenizer, test_df, sample_number=10):\n",
        "    \"\"\"\n",
        "    Evaluates model performance using multiple metrics on test data.\n",
        "\n",
        "    Args:\n",
        "        model: Fine-tuned model to evaluate\n",
        "        tokenizer: Corresponding tokenizer\n",
        "        test_df: Test dataset with 'question' and 'answer' columns\n",
        "        sample_number: Number of samples to evaluate (default: 10)\n",
        "\n",
        "    Returns:\n",
        "        dict: Average scores for BLEU, ROUGE variants, and perplexity\n",
        "    \"\"\"\n",
        "    bleu_scores = []\n",
        "    rouge_scores = []\n",
        "    perplexity_scores = []\n",
        "\n",
        "    # Sample subset of test data for evaluation with fixed random seed\n",
        "    test_df = test_df.sample(n=sample_number, random_state=42)\n",
        "\n",
        "    for _, row in test_df.iterrows():\n",
        "        # Use your predict_answer function for consistency\n",
        "        prediction = predict_answer(row['question'], model, tokenizer)\n",
        "\n",
        "        # Calculate evaluation metrics\n",
        "        bleu = calculate_bleu(row['answer'], prediction)\n",
        "        rouge = calculate_rouge(row['answer'], prediction)\n",
        "        perplexity = calculate_perplexity(model, tokenizer, row['answer'])\n",
        "\n",
        "        # Store scores for averaging\n",
        "        bleu_scores.append(bleu)\n",
        "        rouge_scores.append(rouge)\n",
        "        perplexity_scores.append(perplexity)\n",
        "\n",
        "    # Filter out infinite perplexity values\n",
        "    valid_perplexities = [p for p in perplexity_scores if p != float('inf')]\n",
        "\n",
        "    # Calculate average scores across all samples\n",
        "    avg_bleu = np.mean(bleu_scores)\n",
        "    avg_rouge1 = np.mean([r['rouge1'] for r in rouge_scores])\n",
        "    avg_rouge2 = np.mean([r['rouge2'] for r in rouge_scores])\n",
        "    avg_rougeL = np.mean([r['rougeL'] for r in rouge_scores])\n",
        "    avg_perplexity = np.mean(valid_perplexities) if valid_perplexities else float('inf')\n",
        "\n",
        "    # Display evaluation results\n",
        "    print(f\"BLEU Score: {avg_bleu:.4f}\")\n",
        "    print(f\"ROUGE-1: {avg_rouge1:.4f}\")\n",
        "    print(f\"ROUGE-2: {avg_rouge2:.4f}\")\n",
        "    print(f\"ROUGE-L: {avg_rougeL:.4f}\")\n",
        "    print(f\"Perplexity: {avg_perplexity:.2f}\")\n",
        "\n",
        "    return {\n",
        "        'bleu': avg_bleu,\n",
        "        'rouge1': avg_rouge1,\n",
        "        'rouge2': avg_rouge2,\n",
        "        'rougeL': avg_rougeL,\n",
        "        'perplexity': avg_perplexity\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "XbFGUsI034uY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bf8fc97-62bd-4ade-cf62-6208c3a2b18b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-34-210207630.py:26: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  perplexity = math.exp(loss.numpy())\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score: 0.1148\n",
            "ROUGE-1: 0.3763\n",
            "ROUGE-2: 0.1862\n",
            "ROUGE-L: 0.2976\n",
            "Perplexity: 1.27\n"
          ]
        }
      ],
      "source": [
        "metrics = evaluate_model(model, tokenizer, val_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "qqZr2nol8_fj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54768cac-5f7c-431f-dac5-c0e3c6e10f50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: finance_chatbot_0619_1229\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "def save_model(model, tokenizer, name=\"my_model\"):\n",
        "    \"\"\"\n",
        "    Saves fine-tuned model and tokenizer with timestamped directory name.\n",
        "\n",
        "    Args:\n",
        "        model: Fine-tuned TFT5ForConditionalGeneration model\n",
        "        tokenizer: Corresponding T5 tokenizer\n",
        "        name: Base name for the saved model (default: \"my_model\")\n",
        "\n",
        "    Returns:\n",
        "        None: Creates timestamped directory in ./models/ with saved model files\n",
        "    \"\"\"\n",
        "    # Generate timestamp for unique model versioning\n",
        "    timestamp = datetime.now().strftime(\"%m%d_%H%M\")\n",
        "    full_name = f\"{name}_{timestamp}\"\n",
        "\n",
        "    # Save model and tokenizer to timestamped directory\n",
        "    model.save_pretrained(f\"./models/{full_name}\")\n",
        "    tokenizer.save_pretrained(f\"./models/{full_name}\")\n",
        "\n",
        "    print(f\"Saved: {full_name}\")\n",
        "\n",
        "# Example usage\n",
        "save_model(model, tokenizer, name=\"finance_chatbot\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QdCKeGPYytV4"
      },
      "execution_count": 37,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 31041,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12218342,"sourceType":"datasetVersion","datasetId":7697630},{"sourceId":12226732,"sourceType":"datasetVersion","datasetId":7703366}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U transformers\n!pip install rouge-score nltk","metadata":{"id":"IyL9gZJmryPA","outputId":"704c692a-ac4f-4a0a-b43a-5b7b36107f49","trusted":true,"execution":{"iopub.status.busy":"2025-06-21T21:30:19.567632Z","iopub.execute_input":"2025-06-21T21:30:19.568070Z","iopub.status.idle":"2025-06-21T21:30:37.261158Z","shell.execute_reply.started":"2025-06-21T21:30:19.568045Z","shell.execute_reply":"2025-06-21T21:30:37.260077Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom transformers import AutoTokenizer, TFT5ForConditionalGeneration\nfrom rouge_score import rouge_scorer\nfrom nltk.translate.bleu_score import sentence_bleu","metadata":{"id":"UieOAwLJryPC","trusted":true,"execution":{"iopub.status.busy":"2025-06-21T21:30:37.263039Z","iopub.execute_input":"2025-06-21T21:30:37.263355Z","iopub.status.idle":"2025-06-21T21:31:02.306733Z","shell.execute_reply.started":"2025-06-21T21:30:37.263329Z","shell.execute_reply":"2025-06-21T21:31:02.306130Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataset\n\nThe Warren Buffett Letters Q&A Enhanced Dataset (1998-2024) is a curated collection of question-answer pairs derived from Warren Buffett's annual shareholder letters to Berkshire Hathaway. The dataset spans 26 years of Buffett's investment wisdom and contains structured Q&A pairs extracted from real letter excerpts for conversational learning applications.\n\n**Column Descriptions:**\n- **question**: Financial queries covering investment philosophy, business analysis, market observations, and strategic decisions based on Buffett's letter content\n- **answer**: Expert-level responses reflecting Buffett's investment principles, analytical approach, and decision-making framework\n- **reasoning**: Detailed explanations of the underlying logic and investment principles behind each answer, providing context from Buffett's methodology\n\nThe dataset was chosen for its authentic representation of Warren Buffett's investment philosophy and its structured format that enables effective fine-tuning of sequence-to-sequence models for financial advisory applications in the value investing domain.","metadata":{"id":"f3_Pfnch0OLL"}},{"cell_type":"code","source":"df = pd.read_parquet(\"hf://datasets/eagle0504/warren-buffett-letters-qna-r1-enhanced-1998-2024/data/train-00000-of-00001.parquet\")","metadata":{"id":"G8Ho6IEpryPC","outputId":"eba4e67b-4475-494b-cb2e-1c156316558b","trusted":true,"execution":{"iopub.status.busy":"2025-06-21T21:31:02.307416Z","iopub.execute_input":"2025-06-21T21:31:02.307861Z","iopub.status.idle":"2025-06-21T21:31:03.857112Z","shell.execute_reply.started":"2025-06-21T21:31:02.307841Z","shell.execute_reply":"2025-06-21T21:31:03.856483Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"id":"Qlwhpq6NryPD","outputId":"a5ba4b66-2a8c-4d55-d0f6-da4e1ca3c50e","trusted":true,"execution":{"iopub.status.busy":"2025-06-21T21:31:03.858574Z","iopub.execute_input":"2025-06-21T21:31:03.858819Z","iopub.status.idle":"2025-06-21T21:31:03.879886Z","shell.execute_reply.started":"2025-06-21T21:31:03.858802Z","shell.execute_reply":"2025-06-21T21:31:03.879286Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model\n\nFLAN-T5 is an enhanced version of Google's T5 (Text-to-Text Transfer Transformer) architecture. It extends the original T5 by fine-tuning on over 1,000 additional tasks across multiple languages, creating a more robust foundation model for natural language processing applications.\n\n**Key advantages of FLAN-T5:**\n- **Multilingual capabilities**: Trained on diverse language datasets for broader applicability  \n- **Sequence-to-sequence excellence**: Proven effectiveness for conditional text generation tasks\n- **Large-scale pre-training**: Extensive training corpus ensures strong foundational knowledge\n\nFLAN-T5 was selected as the base model due to its demonstrated performance on sequence-to-sequence tasks and its ability to follow instructions effectively, making it well-suited for question-answering applications.\n\n**Base Model**: [google/flan-t5-base](https://huggingface.co/google/flan-t5-base)","metadata":{"id":"zJp4w0ZU0S8J"}},{"cell_type":"code","source":"def load_tokenizer(model_name=\"google/flan-t5-base\"):\n    \"\"\"\n    Loads tokenizer for T5 model from HuggingFace Hub.\n\n    Args:\n        model_name: HuggingFace model identifier (default: \"google/flan-t5-base\")\n\n    Returns:\n        AutoTokenizer: Pre-trained tokenizer matching the specified model\n    \"\"\"\n    # Load compatible tokenizer for text preprocessing\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    return tokenizer\n","metadata":{"id":"UXfPXQ05ryPE","trusted":true,"execution":{"iopub.status.busy":"2025-06-21T21:31:03.880691Z","iopub.execute_input":"2025-06-21T21:31:03.880920Z","iopub.status.idle":"2025-06-21T21:31:03.886418Z","shell.execute_reply.started":"2025-06-21T21:31:03.880903Z","shell.execute_reply":"2025-06-21T21:31:03.885640Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_model(model_name=\"google/flan-t5-base\"):\n    \"\"\"\n    Loads pre-trained T5 model from HuggingFace Hub.\n\n    Args:\n        model_name: HuggingFace model identifier (default: \"google/flan-t5-base\")\n\n    Returns:\n        TFT5ForConditionalGeneration: Loaded TensorFlow T5 model for conditional generation\n    \"\"\"\n    # Load pre-trained T5 model for sequence-to-sequence tasks\n    model = TFT5ForConditionalGeneration.from_pretrained(model_name)\n    return model","metadata":{"id":"9GSLySfEryPE","trusted":true,"execution":{"iopub.status.busy":"2025-06-21T21:31:03.887233Z","iopub.execute_input":"2025-06-21T21:31:03.887468Z","iopub.status.idle":"2025-06-21T21:31:03.901506Z","shell.execute_reply.started":"2025-06-21T21:31:03.887440Z","shell.execute_reply":"2025-06-21T21:31:03.900776Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Preprocessing\n\nThe dataset undergoes comprehensive preprocessing to ensure high-quality training data for the sequence-to-sequence model. The preprocessing pipeline consists of dataset combination, text cleaning, and quality filtering.\n\n**Dataset Enhancement:**\nThe original financial Q&A data is combined with 75+ out-of-domain questions paired with standardized rejection responses, teaching the model to recognize and politely decline non-financial queries.\n\n**Text Cleaning:**\nRemoves markdown formatting, escaped characters, and annotations while preserving content. Normalizes whitespace and extracts relevant questions from formatted text to create clean, uniform input.\n\n**Quality Filtering:**\nEliminates entries with missing data and enforces minimum length requirements for financial content while preserving all rejection examples for robust domain boundary learning.\n\nThis preprocessing approach creates clean input that enables effective model training while teaching proper domain boundaries and preserving Buffett's investment insights.","metadata":{"id":"8EU9YLaQ2kE_"}},{"cell_type":"code","source":"def preprocess_for_chatbot(text):\n    \"\"\"\n    Preprocesses text for chatbot input by cleaning formatting and extracting questions.\n\n    Args:\n        text: Raw text input (may contain markdown, escape characters, annotations)\n\n    Returns:\n        str: Cleaned text ready for chatbot processing\n    \"\"\"\n    # Handle missing or null values\n    if pd.isna(text):\n        return \"\"\n\n    text = str(text)\n\n    # Replace escaped characters for readability\n    text = text.replace(\"\\\\n\", \" \").replace(\"\\\\t\", \" \").replace(\"\\\\'\", \"'\").replace('\\\\\"', '\"')\n\n    # Extract first question from bold markdown format\n    questions = re.findall(r'\\*\\*\"([^\"]*?)\"\\*\\*', text)\n    if questions:\n        return questions[0].strip()\n\n    # Remove markdown formatting\n    text = re.sub(r'\\*\\*(.*?)\\*\\*', r'\\1', text)  # **text** → text\n    text = re.sub(r'\\*(.*?)\\*', r'\\1', text)      # *text* → text\n\n    # Remove annotations in parentheses\n    text = re.sub(r'\\*\\([^)]*\\)\\*', '', text)\n\n    # Normalize whitespace\n    text = re.sub(r'\\s+', ' ', text).strip()\n\n    # Remove surrounding quotes if they wrap the entire text\n    if text.startswith('\"') and text.endswith('\"') and text.count('\"') == 2:\n        text = text[1:-1].strip()\n\n    return text","metadata":{"id":"Asa0a_AQryPD","trusted":true,"execution":{"iopub.status.busy":"2025-06-21T21:31:03.902257Z","iopub.execute_input":"2025-06-21T21:31:03.902503Z","iopub.status.idle":"2025-06-21T21:31:03.914991Z","shell.execute_reply.started":"2025-06-21T21:31:03.902472Z","shell.execute_reply":"2025-06-21T21:31:03.914380Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_chatbot_dataset(df):\n    \"\"\"\n    Preprocesses the entire chatbot dataset by cleaning text and filtering entries.\n\n    Args:\n        df: DataFrame containing question-answer pairs with optional reasoning\n\n    Returns:\n        DataFrame: Cleaned dataset ready for model training\n    \"\"\"\n    # Apply text cleaning to all relevant columns\n    for col in ['question', 'reasoning', 'answer']:\n        if col in df.columns:\n            df[col] = df[col].apply(preprocess_for_chatbot)\n\n    # Remove entries with missing critical data\n    df = df.dropna(subset=['question', 'answer'])\n\n    # Filter out entries that are too short to be meaningful\n    df = df[df['question'].str.len() > 5]\n    df = df[df['answer'].str.len() > 10]\n\n    print(f\"Dataset ready: {len(df)} samples\")\n\n    # Display sample data for verification\n    if len(df) > 0:\n        print(f\"\\nSample preserved question:\")\n        print(f\"'{df['question'].iloc[0]}'\")\n        print(f\"\\nSample preserved answer:\")\n        print(f\"'{df['answer'].iloc[0]}'\")\n        print(f\"\\nSample preserved reasoning:\")\n        print(f\"'{df['reasoning'].iloc[0]}'\")\n\n    return df","metadata":{"id":"sGmGb01FryPD","trusted":true,"execution":{"iopub.status.busy":"2025-06-21T21:31:03.915642Z","iopub.execute_input":"2025-06-21T21:31:03.915867Z","iopub.status.idle":"2025-06-21T21:31:03.934376Z","shell.execute_reply.started":"2025-06-21T21:31:03.915844Z","shell.execute_reply":"2025-06-21T21:31:03.933662Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndef split_train_val(df, val_size=0.1):\n    \"\"\"\n    Splits dataset into training and validation sets.\n\n    Args:\n        df: Input DataFrame to split\n        val_size: Fraction for validation set (default: 0.2)\n\n    Returns:\n        tuple: (train_df, val_df) - Training and validation DataFrames\n    \"\"\"\n    # Split with fixed random seed\n    train_df, val_df = train_test_split(df, test_size=val_size, random_state=42)\n\n    return train_df, val_df","metadata":{"id":"H1FURQvvryPF","trusted":true,"execution":{"iopub.status.busy":"2025-06-21T21:31:03.935290Z","iopub.execute_input":"2025-06-21T21:31:03.935753Z","iopub.status.idle":"2025-06-21T21:31:03.950938Z","shell.execute_reply.started":"2025-06-21T21:31:03.935733Z","shell.execute_reply":"2025-06-21T21:31:03.950249Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tokenize_data(df, tokenizer, batch_size=10):\n    \"\"\"\n    Tokenizes question-answer pairs and returns a batched tf.data.Dataset.\n\n    Args:\n        df: DataFrame with 'question' and 'answer' columns\n        tokenizer: Pre-trained tokenizer (e.g., from transformers library)\n        batch_size: Number of samples per batch (default: 4)\n\n    Returns:\n        tf.data.Dataset: Batched dataset with input_ids, attention_mask, and labels\n    \"\"\"\n    inputs = []\n    labels = []\n    attention_masks = []\n\n    for index, row in df.iterrows():\n        # Add context prompt during training (same as inference)\n        input_text = f\"Answer this financial question based on Warren Buffett's principles: {row['question']}\"\n\n        question_tokens = tokenizer(\n            input_text,  # Use formatted input instead of raw question\n            max_length=256,\n            padding='max_length',\n            truncation=True,\n            return_tensors='tf'\n        )\n        inputs.append(question_tokens['input_ids'][0])\n        attention_masks.append(question_tokens['attention_mask'][0])\n\n        # Tokenize answer\n        answer_tokens = tokenizer(\n            row['answer'],\n            max_length=256,\n            padding='max_length',\n            truncation=True,\n            return_tensors='tf'\n        )\n        labels.append(answer_tokens['input_ids'][0])\n\n    # Convert to tensors\n    inputs = tf.stack(inputs)\n    attention_masks = tf.stack(attention_masks)\n    labels = tf.stack(labels)\n\n    # Create and return batched dataset\n    return tf.data.Dataset.from_tensor_slices({\n        'input_ids': inputs,\n        'attention_mask': attention_masks,\n        'labels': labels\n    }).batch(batch_size)","metadata":{"id":"EI8C8kJ4ryPF","trusted":true,"execution":{"iopub.status.busy":"2025-06-21T21:31:03.953829Z","iopub.execute_input":"2025-06-21T21:31:03.954074Z","iopub.status.idle":"2025-06-21T21:31:03.967401Z","shell.execute_reply.started":"2025-06-21T21:31:03.954060Z","shell.execute_reply":"2025-06-21T21:31:03.966802Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the out-of-domain data\nout_of_domain_df = pd.read_csv('/kaggle/input/out-of-domain-dataset/out_of_domain_training_data.csv')\n\n# Combine both datasets\nenhanced_df = pd.concat([df, out_of_domain_df], ignore_index=True)\n\n# Shuffle the combined dataset\nenhanced_df = enhanced_df.sample(frac=1, random_state=42).reset_index(drop=True)","metadata":{"id":"EGPI8jxt0F0O","trusted":true,"execution":{"iopub.status.busy":"2025-06-21T21:31:03.968059Z","iopub.execute_input":"2025-06-21T21:31:03.968354Z","iopub.status.idle":"2025-06-21T21:31:04.008632Z","shell.execute_reply.started":"2025-06-21T21:31:03.968331Z","shell.execute_reply":"2025-06-21T21:31:04.008074Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = preprocess_chatbot_dataset(enhanced_df)","metadata":{"id":"NbG0gvmJ9k37","outputId":"d9640ab4-6ec3-47fe-e3fb-1d6700bf321a","trusted":true,"execution":{"iopub.status.busy":"2025-06-21T21:31:04.009203Z","iopub.execute_input":"2025-06-21T21:31:04.009386Z","iopub.status.idle":"2025-06-21T21:31:05.305822Z","shell.execute_reply.started":"2025-06-21T21:31:04.009372Z","shell.execute_reply":"2025-06-21T21:31:05.305057Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df, val_df = split_train_val(df)\n\nprint(f\"Train samples: {len(train_df)}\")\nprint(f\"Val samples: {len(val_df)}\")","metadata":{"id":"Dkyfiak8ryPF","outputId":"84c10215-4e7a-4844-e2d0-c0883edec76a","trusted":true,"execution":{"iopub.status.busy":"2025-06-21T21:31:05.306711Z","iopub.execute_input":"2025-06-21T21:31:05.307000Z","iopub.status.idle":"2025-06-21T21:31:05.314008Z","shell.execute_reply.started":"2025-06-21T21:31:05.306975Z","shell.execute_reply":"2025-06-21T21:31:05.313112Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(train_dict, val_dict, learning_rate=5.8e-5, callbacks=[], epochs=40):\n    \"\"\"\n    Fine-tunes FLAN-T5 model on question-answer pairs and returns history.\n    \n    Args:\n        train_dict: Training dataset (tf.data.Dataset with input_ids, attention_mask, labels)\n        val_dict: Validation dataset (tf.data.Dataset with same structure)\n        learning_rate: Adam optimizer learning rate (default: 6e-5)\n        callbacks: List of Keras callbacks for training monitoring (default: [])\n        epochs: Number of training epochs (default: 30)\n    \n    Returns:\n        tuple: (model, history) - Fine-tuned model and training history\n    \"\"\"\n    # Load pre-trained FLAN-T5 base model\n    model = load_model()\n    \n    # Configure Adam optimizer with specified learning rate\n    optimizer = tf.keras.optimizers.AdamW(\n        learning_rate=learning_rate,\n        weight_decay=0.01\n    )\n    model.compile(optimizer=optimizer)\n    \n    # Fine-tune model with callbacks for training monitoring\n    history = model.fit(\n        train_dict,\n        validation_data=val_dict,\n        epochs=epochs,\n        callbacks=callbacks\n    )\n    \n    return model, history","metadata":{"id":"CNHZWGM-ryPF","trusted":true,"execution":{"iopub.status.busy":"2025-06-21T21:31:05.314946Z","iopub.execute_input":"2025-06-21T21:31:05.315241Z","iopub.status.idle":"2025-06-21T21:31:05.372165Z","shell.execute_reply.started":"2025-06-21T21:31:05.315220Z","shell.execute_reply":"2025-06-21T21:31:05.371413Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n\ndef plot_training_history(history):\n    \"\"\"\n    Simple plot of training and validation loss.\n    \n    Args:\n        history: Keras training history object\n    \"\"\"\n    plt.figure(figsize=(10, 6))\n    \n    # Plot training loss\n    plt.plot(history.history['loss'], label='Training Loss', color='blue', linewidth=2)\n    \n    # Plot validation loss if available\n    if 'val_loss' in history.history:\n        plt.plot(history.history['val_loss'], label='Validation Loss', color='red', linewidth=2)\n    \n    plt.title('Training History', fontsize=16, fontweight='bold')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.show()\n    \n    # Print final results\n    final_train_loss = history.history['loss'][-1]\n    print(f\"Final Training Loss: {final_train_loss:.4f}\")\n    \n    if 'val_loss' in history.history:\n        final_val_loss = history.history['val_loss'][-1]\n        print(f\"Final Validation Loss: {final_val_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T21:31:05.372902Z","iopub.execute_input":"2025-06-21T21:31:05.373679Z","iopub.status.idle":"2025-06-21T21:31:05.386954Z","shell.execute_reply.started":"2025-06-21T21:31:05.373656Z","shell.execute_reply":"2025-06-21T21:31:05.386216Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_simple_callbacks():\n    \"\"\"\n    Creates standard Keras callbacks for training optimization and monitoring.\n    Returns:\n        list: Configured callbacks for early stopping, learning rate reduction, and model checkpointing\n    \"\"\"\n    # Training hyperparameters - VERY aggressive to catch the plateau shown in graph\n    PATIENCE = 2             # Back to original - very aggressive\n    LR_REDUCTION_FACTOR = 0.2 # More aggressive than original 0.3\n    LR_PATIENCE = 1           # Back to original - quick LR reduction\n    MIN_LR = 1e-8            # Keep original - allows very small LRs\n    MIN_DELTA = 0.0005       # Small but realistic - matches the small changes visible in graph\n    RESTORE_BEST = True      # Keep this - important for avoiding overfitting\n    \n    callbacks = [\n        # Prevent overfitting by stopping training when validation loss plateaus\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=PATIENCE,\n            restore_best_weights=RESTORE_BEST,\n            min_delta=MIN_DELTA,\n            verbose=1,\n            mode='min'           # Explicitly specify we want to minimize val_loss\n        ),\n        \n        # Adaptive learning rate reduction when training stagnates\n        tf.keras.callbacks.ReduceLROnPlateau(\n            initial_lr = 5.8e-5,\n            monitor='val_loss',\n            factor=LR_REDUCTION_FACTOR,\n            patience=LR_PATIENCE,\n            min_lr=MIN_LR,\n            min_delta=MIN_DELTA,\n            cooldown=2,              # Increased from 1 - more cooldown time\n            verbose=1,\n            mode='min'\n        ),\n        \n        # Save best performing model during training\n        tf.keras.callbacks.ModelCheckpoint(\n            filepath='./best_model.keras',\n            monitor='val_loss',\n            save_best_only=True,\n            save_weights_only=False,  # Save full model, not just weights\n            verbose=1,\n            mode='min'\n        ),\n        \n        # Optional: Add very aggressive early stopping as backup\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=1,              # Extremely aggressive - stops after 1 epoch of no improvement\n            restore_best_weights=True,\n            min_delta=0.0001,        # Very small threshold for final check\n            verbose=1,\n            mode='min'\n        )\n    ]\n    \n    return callbacks","metadata":{"id":"T2rB_Vuf-wyI","trusted":true,"execution":{"iopub.status.busy":"2025-06-21T21:31:05.387809Z","iopub.execute_input":"2025-06-21T21:31:05.387999Z","iopub.status.idle":"2025-06-21T21:31:05.403514Z","shell.execute_reply.started":"2025-06-21T21:31:05.387985Z","shell.execute_reply":"2025-06-21T21:31:05.402755Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = load_tokenizer()\n\ntrain_dict = tokenize_data(train_df, tokenizer)\nval_dict = tokenize_data(val_df, tokenizer)","metadata":{"id":"rQfbsPm8ryPF","trusted":true,"execution":{"iopub.status.busy":"2025-06-21T21:31:05.404391Z","iopub.execute_input":"2025-06-21T21:31:05.404641Z","iopub.status.idle":"2025-06-21T21:31:32.245519Z","shell.execute_reply.started":"2025-06-21T21:31:05.404621Z","shell.execute_reply":"2025-06-21T21:31:32.244808Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"callbacks = get_simple_callbacks()\n\nmodel, history = train_model(train_dict, val_dict, callbacks=callbacks)","metadata":{"id":"vt6MY891ryPF","outputId":"ac450c2a-bbbe-4b98-f868-5891bb1701c7","trusted":true,"execution":{"iopub.status.busy":"2025-06-21T21:31:32.246442Z","iopub.execute_input":"2025-06-21T21:31:32.246670Z","execution_failed":"2025-06-21T21:36:41.603Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_training_history(history)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-21T21:36:41.603Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"question = \"How often do you review your portfolio\"","metadata":{"id":"Vr5TT4rrryPF","trusted":true,"execution":{"execution_failed":"2025-06-21T21:36:41.603Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_answer(question, model, tokenizer):\n    \"\"\"\n    Generates answers to financial questions using fine-tuned FLAN-T5 model.\n\n    Args:\n        question: Input question text\n        model: Fine-tuned TFT5ForConditionalGeneration model\n        tokenizer: Corresponding T5 tokenizer\n\n    Returns:\n        str: Generated answer following Warren Buffett's investment principles\n    \"\"\"\n    # Format input with same context prompt used during training\n    input_text = f\"Answer this financial question based on Warren Buffett's principles: {question}\"\n\n    # Tokenize input with padding and truncation for consistent format\n    input_tokens = tokenizer(\n        input_text,\n        return_tensors=\"tf\",\n        max_length=256,\n        padding='max_length',\n        truncation=True\n    )\n\n    # Generate response using beam search for higher quality output\n    generated_tokens = model.generate(\n        input_tokens[\"input_ids\"],\n        attention_mask=input_tokens[\"attention_mask\"],\n        max_length=256,\n        num_beams=4,\n        early_stopping=True\n    )\n\n    # Decode generated tokens to readable text\n    predicted_answer = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n\n    return predicted_answer\n\n# Example usage\npredicted_answer = predict_answer(question, model, tokenizer)\nprint(f\"\\nQuestion: {question}\")\nprint(f\"Predicted Answer: {predicted_answer}\")","metadata":{"id":"eZPIakPR34uX","outputId":"181ed849-e02a-4ebe-fcdd-655476a85332","trusted":true,"execution":{"execution_failed":"2025-06-21T21:36:41.603Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_bleu(reference, candidate):\n    \"\"\"\n    Calculates BLEU score to measure similarity between reference and generated text.\n\n    Args:\n        reference: Ground truth answer text\n        candidate: Model-generated answer text\n\n    Returns:\n        float: BLEU score between 0 and 1 (higher = better similarity)\n    \"\"\"\n    # Tokenize texts by splitting on whitespace\n    reference_tokens = reference.split()\n    candidate_tokens = candidate.split()\n\n    # Calculate sentence-level BLEU score\n    return sentence_bleu([reference_tokens], candidate_tokens)","metadata":{"id":"1s4LfG6z34uY","trusted":true,"execution":{"execution_failed":"2025-06-21T21:36:41.604Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_rouge(reference, candidate):\n    \"\"\"\n    Calculates ROUGE scores to evaluate text generation quality.\n\n    Args:\n        reference: Ground truth answer text\n        candidate: Model-generated answer text\n\n    Returns:\n        dict: ROUGE scores with keys 'rouge1', 'rouge2', 'rougeL' (0-1 scale, higher = better)\n    \"\"\"\n    # Initialize ROUGE scorer with stemming for better matching\n    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n\n    # Calculate all ROUGE variants\n    scores = scorer.score(reference, candidate)\n\n    # Return F1 scores for each ROUGE metric\n    return {\n        'rouge1': scores['rouge1'].fmeasure,   # Unigram overlap\n        'rouge2': scores['rouge2'].fmeasure,   # Bigram overlap\n        'rougeL': scores['rougeL'].fmeasure    # Longest common subsequence\n    }","metadata":{"id":"9QFcp45c34uY","trusted":true,"execution":{"execution_failed":"2025-06-21T21:36:41.604Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import math\n\ndef calculate_perplexity(model, tokenizer, text, max_length=256):\n    \"\"\"\n    Calculates perplexity to measure how well the model predicts the given text.\n\n    Args:\n        model: Fine-tuned language model\n        tokenizer: Corresponding tokenizer\n        text: Input text to evaluate\n        max_length: Maximum sequence length for tokenization (default: 256)\n\n    Returns:\n        float: Perplexity score (lower = better, inf if calculation fails)\n    \"\"\"\n    try:\n        # Tokenize input text with truncation for consistent length\n        inputs = tokenizer(text, return_tensors='tf', max_length=max_length, truncation=True)\n        input_ids = inputs['input_ids']\n\n        # Calculate loss using input as both input and target (language modeling)\n        outputs = model(input_ids, labels=input_ids)\n        loss = outputs.loss\n\n        # Convert cross-entropy loss to perplexity\n        perplexity = math.exp(loss.numpy())\n        return perplexity\n\n    except:\n        # Return infinity if calculation fails (e.g., empty text, model errors)\n        return float('inf')","metadata":{"id":"pmW0JoeYEVRA","trusted":true,"execution":{"execution_failed":"2025-06-21T21:36:41.604Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_model(model, tokenizer, test_df, sample_number=10):\n    \"\"\"\n    Evaluates model performance using multiple metrics on test data.\n\n    Args:\n        model: Fine-tuned model to evaluate\n        tokenizer: Corresponding tokenizer\n        test_df: Test dataset with 'question' and 'answer' columns\n        sample_number: Number of samples to evaluate (default: 10)\n\n    Returns:\n        dict: Average scores for BLEU, ROUGE variants, and perplexity\n    \"\"\"\n    bleu_scores = []\n    rouge_scores = []\n    perplexity_scores = []\n\n    # Sample subset of test data for evaluation with fixed random seed\n    test_df = test_df.sample(n=sample_number, random_state=42)\n\n    for _, row in test_df.iterrows():\n        # Use your predict_answer function for consistency\n        prediction = predict_answer(row['question'], model, tokenizer)\n\n        # Calculate evaluation metrics\n        bleu = calculate_bleu(row['answer'], prediction)\n        rouge = calculate_rouge(row['answer'], prediction)\n        perplexity = calculate_perplexity(model, tokenizer, row['answer'])\n\n        # Store scores for averaging\n        bleu_scores.append(bleu)\n        rouge_scores.append(rouge)\n        perplexity_scores.append(perplexity)\n\n    # Filter out infinite perplexity values\n    valid_perplexities = [p for p in perplexity_scores if p != float('inf')]\n\n    # Calculate average scores across all samples\n    avg_bleu = np.mean(bleu_scores)\n    avg_rouge1 = np.mean([r['rouge1'] for r in rouge_scores])\n    avg_rouge2 = np.mean([r['rouge2'] for r in rouge_scores])\n    avg_rougeL = np.mean([r['rougeL'] for r in rouge_scores])\n    avg_perplexity = np.mean(valid_perplexities) if valid_perplexities else float('inf')\n\n    # Display evaluation results\n    print(f\"BLEU Score: {avg_bleu:.4f}\")\n    print(f\"ROUGE-1: {avg_rouge1:.4f}\")\n    print(f\"ROUGE-2: {avg_rouge2:.4f}\")\n    print(f\"ROUGE-L: {avg_rougeL:.4f}\")\n    print(f\"Perplexity: {avg_perplexity:.2f}\")\n\n    return {\n        'bleu': avg_bleu,\n        'rouge1': avg_rouge1,\n        'rouge2': avg_rouge2,\n        'rougeL': avg_rougeL,\n        'perplexity': avg_perplexity\n    }","metadata":{"id":"4MGAR0Ao34uY","trusted":true,"execution":{"execution_failed":"2025-06-21T21:36:41.604Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"metrics = evaluate_model(model, tokenizer, val_df)","metadata":{"id":"XbFGUsI034uY","outputId":"0bf8fc97-62bd-4ade-cf62-6208c3a2b18b","trusted":true,"execution":{"execution_failed":"2025-06-21T21:36:41.604Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datetime import datetime\n\ndef save_model(model, tokenizer, name=\"my_model\"):\n    \"\"\"\n    Saves fine-tuned model and tokenizer with timestamped directory name.\n\n    Args:\n        model: Fine-tuned TFT5ForConditionalGeneration model\n        tokenizer: Corresponding T5 tokenizer\n        name: Base name for the saved model (default: \"my_model\")\n\n    Returns:\n        None: Creates timestamped directory in ./models/ with saved model files\n    \"\"\"\n    # Generate timestamp for unique model versioning\n    timestamp = datetime.now().strftime(\"%m%d_%H%M\")\n    full_name = f\"{name}_{timestamp}\"\n\n    # Save model and tokenizer to timestamped directory\n    model.save_pretrained(f\"./models/{full_name}\")\n    tokenizer.save_pretrained(f\"./models/{full_name}\")\n\n    print(f\"Saved: {full_name}\")\n\n# Example usage\nsave_model(model, tokenizer, name=\"finance_chatbot\")","metadata":{"id":"qqZr2nol8_fj","outputId":"54768cac-5f7c-431f-dac5-c0e3c6e10f50","trusted":true,"execution":{"execution_failed":"2025-06-21T21:36:41.604Z"}},"outputs":[],"execution_count":null}]}